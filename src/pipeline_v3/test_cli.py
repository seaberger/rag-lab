"""
CLI Tests for Pipeline v3 - Phase 3 Testing

Tests for the command-line interface covering all major commands and operations.
"""

import asyncio
import json
import tempfile
from pathlib import Path
from unittest.mock import AsyncMock, MagicMock, patch
import sys

# Add parent directory for imports
sys.path.insert(0, str(Path(__file__).parent))

from cli.management import PipelineCLI


class TestCLI:
    """Test suite for CLI functionality."""
    
    def setup_method(self):
        """Set up test environment."""
        self.temp_dir = tempfile.mkdtemp()
        self.test_config = {
            'storage': {
                'base_path': self.temp_dir,
                'cache_path': f"{self.temp_dir}/cache",
                'registry_db_path': f"{self.temp_dir}/registry.db"
            },
            'queue': {
                'max_workers': 2
            }
        }
    
    def create_test_cli(self):
        """Create CLI instance with mocked dependencies."""
        with patch('cli.management.CORE_AVAILABLE', True), \
             patch('cli.management.PIPELINE_AVAILABLE', True):
            
            cli = PipelineCLI()
            cli.config = MagicMock()
            cli.config.get = lambda key, default=None: self.test_config.get(key.split('.')[0], {}).get(key.split('.')[1], default) if '.' in key else default
            cli.config.storage_config = self.test_config['storage']
            
            # Mock pipeline components
            cli.pipeline = AsyncMock()
            cli.queue = AsyncMock()
            cli.registry = AsyncMock()
            cli.index_manager = AsyncMock()
            cli.monitor = MagicMock()
            
            return cli
    
    def test_parser_creation(self):
        """Test that argument parser is created correctly."""
        cli = self.create_test_cli()
        parser = cli.create_parser()
        
        # Test main parser
        assert parser.prog == 'pipeline'
        assert 'Production Document Processing Pipeline v3' in parser.description
        
        # Test that all main commands are present
        subparsers_actions = [
            action for action in parser._actions 
            if isinstance(action, type(parser._subparsers))
        ]
        assert len(subparsers_actions) == 1
        
        subparsers = subparsers_actions[0]
        command_names = list(subparsers.choices.keys())
        expected_commands = ['add', 'update', 'remove', 'search', 'queue', 'status', 'maintenance', 'config']
        
        for cmd in expected_commands:
            assert cmd in command_names, f"Command '{cmd}' not found in parser"
    
    def test_parse_metadata(self):
        """Test metadata parsing functionality."""
        cli = self.create_test_cli()
        
        # Test valid metadata
        metadata_list = ['type=datasheet', 'version=1.0', 'complex={"key": "value"}']
        result = cli._parse_metadata(metadata_list)
        
        assert result['type'] == 'datasheet'
        assert result['version'] == '1.0'
        assert result['complex'] == {"key": "value"}
        
        # Test empty metadata
        assert cli._parse_metadata([]) == {}
        assert cli._parse_metadata(None) == {}
    
    def test_format_output(self):
        """Test output formatting."""
        cli = self.create_test_cli()
        
        # Test JSON format
        data = {'key': 'value', 'number': 42}
        json_output = cli._format_output(data, json_format=True)
        assert json.loads(json_output) == data
        
        # Test plain format
        plain_output = cli._format_output(data, json_format=False)
        assert 'key: value' in plain_output
        assert 'number: 42' in plain_output
        
        # Test list format
        list_data = ['item1', 'item2', 'item3']
        list_output = cli._format_output(list_data, json_format=False)
        assert 'item1' in list_output
        assert 'item2' in list_output
    
    async def test_handle_add(self):
        """Test document addition handling."""
        cli = self.create_test_cli()
        
        # Mock arguments
        args = MagicMock()
        args.paths = ['test.pdf']
        args.metadata = ['type=datasheet']
        args.force = False
        args.index_type = 'both'
        args.json = False
        
        # Mock pipeline response
        cli.pipeline.process_document.return_value = {'status': 'success'}
        
        # Test execution
        await cli.handle_add(args)
        
        # Verify pipeline was called correctly
        cli.pipeline.process_document.assert_called_once_with(
            'test.pdf',
            metadata={'type': 'datasheet'},
            force=False,
            index_types='both'
        )
    
    async def test_handle_search(self):
        """Test search functionality."""
        cli = self.create_test_cli()
        
        # Mock arguments
        args = MagicMock()
        args.query = 'laser sensors'
        args.type = 'hybrid'
        args.top_k = 5
        args.filter = None
        args.json = False
        
        # Mock search results
        cli.pipeline.search.return_value = [
            {
                'source': 'doc1.pdf',
                'score': 0.95,
                'content': 'Laser sensor specifications and capabilities...'
            },
            {
                'source': 'doc2.pdf', 
                'score': 0.87,
                'content': 'Advanced laser measurement technologies...'
            }
        ]
        
        # Test execution
        await cli.handle_search(args)
        
        # Verify search was called correctly
        cli.pipeline.search.assert_called_once_with(
            'laser sensors',
            search_type='hybrid',
            top_k=5,
            filter_dict=None
        )
    
    async def test_handle_queue_operations(self):
        """Test queue management operations."""
        cli = self.create_test_cli()
        
        # Test queue start
        args = MagicMock()
        args.queue_action = 'start'
        args.workers = 4
        
        await cli.handle_queue(args)
        cli.queue.start.assert_called_once_with(max_workers=4)
        
        # Test queue stop
        args.queue_action = 'stop'
        args.wait = True
        
        await cli.handle_queue(args)
        cli.queue.stop.assert_called_with(wait_for_completion=True)
        
        # Test queue status
        args.queue_action = 'status'
        args.detailed = False
        args.json = False
        cli.queue.get_status.return_value = {
            'state': 'running',
            'pending_jobs': 5,
            'active_jobs': 2
        }
        
        await cli.handle_queue(args)
        cli.queue.get_status.assert_called()
    
    async def test_handle_status(self):
        """Test system status reporting."""
        cli = self.create_test_cli()
        
        # Mock arguments
        args = MagicMock()
        args.detailed = False
        args.json = False
        
        # Mock component status
        cli.pipeline.get_status.return_value = {'state': 'ready'}
        cli.queue.get_status.return_value = {'state': 'running'}
        cli.registry.get_statistics.return_value = {'total_documents': 10}
        cli.index_manager.get_status.return_value = {'healthy_indexes': 2}
        
        # Test execution
        await cli.handle_status(args)
        
        # Verify all components were queried
        cli.pipeline.get_status.assert_called_once()
        cli.queue.get_status.assert_called_once()
        cli.registry.get_statistics.assert_called_once()
        cli.index_manager.get_status.assert_called_once()
    
    async def test_handle_maintenance(self):
        """Test maintenance operations."""
        cli = self.create_test_cli()
        
        # Test repair operation
        args = MagicMock()
        args.repair = True
        args.cleanup = False
        args.consistency_check = False
        
        cli.index_manager.repair_indexes.return_value = 'repair_completed'
        
        await cli.handle_maintenance(args)
        cli.index_manager.repair_indexes.assert_called_once()
        
        # Test consistency check
        args.repair = False
        args.consistency_check = True
        
        cli.index_manager.verify_consistency.return_value = 'consistency_ok'
        
        await cli.handle_maintenance(args)
        cli.index_manager.verify_consistency.assert_called_once()
    
    async def test_handle_config(self):
        """Test configuration management."""
        cli = self.create_test_cli()
        
        # Test config list
        args = MagicMock()
        args.config_action = 'list'
        args.json = False
        
        cli.config.to_dict.return_value = {'setting1': 'value1', 'setting2': 'value2'}
        
        await cli.handle_config(args)
        cli.config.to_dict.assert_called_once()
        
        # Test config get
        args.config_action = 'get'
        args.key = 'setting1'
        args.json = False
        
        cli.config.get.return_value = 'value1'
        
        await cli.handle_config(args)
        # Note: get is called multiple times during setup, so we just verify it exists
        assert hasattr(cli.config, 'get')
        
        # Test config set
        args.config_action = 'set'
        args.key = 'setting1'
        args.value = 'new_value'
        
        await cli.handle_config(args)
        cli.config.set.assert_called_once_with('setting1', 'new_value')
        cli.config.save.assert_called_once()
    
    def test_error_handling(self):
        """Test error handling in CLI operations."""
        cli = self.create_test_cli()
        
        # Test metadata parsing with invalid input
        invalid_metadata = ['invalid_format', 'key=value']
        result = cli._parse_metadata(invalid_metadata)
        
        # Should skip invalid entries but process valid ones
        assert 'key' in result
        assert result['key'] == 'value'
    
    async def test_json_output_mode(self):
        """Test JSON output formatting."""
        cli = self.create_test_cli()
        
        # Mock arguments for search with JSON output
        args = MagicMock()
        args.query = 'test'
        args.type = 'vector'
        args.top_k = 3
        args.filter = None
        args.json = True
        
        # Mock search results
        test_results = [{'source': 'test.pdf', 'score': 0.9}]
        cli.pipeline.search.return_value = test_results
        
        # Capture output (in real implementation, this would print JSON)
        await cli.handle_search(args)
        
        # Verify search was called
        cli.pipeline.search.assert_called_once()


def run_tests():
    """Run all CLI tests."""
    test_instance = TestCLI()
    
    print("Running CLI Tests...")
    
    # Synchronous tests
    try:
        test_instance.setup_method()
        test_instance.test_parser_creation()
        print("✓ Parser creation test passed")
        
        test_instance.test_parse_metadata() 
        print("✓ Metadata parsing test passed")
        
        test_instance.test_format_output()
        print("✓ Output formatting test passed")
        
        test_instance.test_error_handling()
        print("✓ Error handling test passed")
        
    except Exception as e:
        print(f"✗ Synchronous test failed: {e}")
        return False
    
    # Asynchronous tests
    async def run_async_tests():
        try:
            test_instance.setup_method()
            
            await test_instance.test_handle_add()
            print("✓ Add command test passed")
            
            await test_instance.test_handle_search()
            print("✓ Search command test passed")
            
            await test_instance.test_handle_queue_operations()
            print("✓ Queue operations test passed")
            
            await test_instance.test_handle_status()
            print("✓ Status command test passed")
            
            await test_instance.test_handle_maintenance()
            print("✓ Maintenance command test passed")
            
            await test_instance.test_handle_config()
            print("✓ Config command test passed")
            
            await test_instance.test_json_output_mode()
            print("✓ JSON output test passed")
            
            return True
            
        except Exception as e:
            print(f"✗ Asynchronous test failed: {e}")
            return False
    
    async_result = asyncio.run(run_async_tests())
    
    if async_result:
        print("\n🎉 All CLI tests passed! Phase 3 CLI implementation complete.")
        return True
    else:
        print("\n❌ Some CLI tests failed.")
        return False


if __name__ == "__main__":
    success = run_tests()
    sys.exit(0 if success else 1)